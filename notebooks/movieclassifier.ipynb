{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd058471ff71dffc31aa3f4eb5aa8bca7f70d6dcded1512caf002265e3cc8042e4a",
   "display_name": "Python 3.9.4 64-bit ('movieclassifier-tI1HZEoP-py3.9': poetry)"
  },
  "metadata": {
   "interpreter": {
    "hash": "d5db8f89a013d07fccae8607c506956f68956ac46c0a3a09b0fd4fdb7d869c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MLOps - Movieclassifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Applying Machine Learning To Sentiment Analysis using IMDB sentiment dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import Relevant Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import os\n",
    "import pyprind"
   ]
  },
  {
   "source": [
    "## Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We will  assemble the individual\n",
    "text documents from the decompressed download archive into a single CSV file.\n",
    "In the following code section, we will be reading the movie reviews into a pandas DataFrame object"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# change the `basepath` to the directory of the\n",
    "# unzipped movie dataset\n",
    "\n",
    "basepath = '../data/raw/aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:17\n"
     ]
    }
   ]
  },
  {
   "source": [
    "In the preceding code, we first initialized a new progress bar object, pbar, with 50,000 iterations, which was the number of documents we were going to read in. Using the nested for loops, we iterated over the train and test subdirectories in the main aclImdb directory and read the individual text files from the pos and neg subdirectories that we eventually appended to the df pandas DataFrame, together with an integer class label (1 = positive and 0 = negative)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Since the class labels in the assembled dataset are sorted, we will now shuffle DataFrame using the permutation function from the np.random submoduleâ€”\n",
    "this will be useful to split the dataset into training and test datasets in later sections, when we will stream the data from our local drive directly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "seed = 28\n",
    "np.random.seed(seed)\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "Lets see the dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  review  sentiment\n",
       "18924  Stu Ungar is considered by many to be the grea...          0\n",
       "5868   This movie kicks ass, bar none. Bam and his cr...          1\n",
       "33809  I got to see this on the plane to NZ last week...          1\n",
       "1768   i consider this movie as one of the most inter...          1\n",
       "44063  This was the only time I ever walked out on a ...          0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18924</th>\n      <td>Stu Ungar is considered by many to be the grea...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5868</th>\n      <td>This movie kicks ass, bar none. Bam and his cr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33809</th>\n      <td>I got to see this on the plane to NZ last week...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1768</th>\n      <td>i consider this movie as one of the most inter...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44063</th>\n      <td>This was the only time I ever walked out on a ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "For our own convenience, we will also store the assembled and shuffled movie review dataset as a CSV file:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/interim/movie_data.csv', index=False)"
   ]
  },
  {
   "source": [
    "## Remove special charactars"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'A sequel of sorts is in the works.<br /><br />7/10'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.loc[15, 'review'][-50:]"
   ]
  },
  {
   "source": [
    "As you can see here, the text contains HTML markup as well as punctuation and other non-letter characters. While HTML markup does not contain many useful semantics, punctuation marks can represent useful, additional information in certain NLP contexts. However, for simplicity, we will now remove all punctuation marks except for emoticon characters, such as :), since those are certainly useful for sentiment analysis. To accomplish this task, we will use Python's regular expression (regex) library, re, as shown here:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'a sequel of sorts is in the works 7 10'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\n",
    "preprocessor(df.loc[15, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "\n",
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "source": [
    "## Tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "source": [
    "In order to remove stop-words from the movie reviews, we will use the set of 127 English stop-words that is available from the NLTK library, which can be obtained by calling the nltk.download function:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/fallou/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:]\n",
    "if w not in stop]"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Online Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('\"Stu Ungar is considered by many to be the greatest poker / gin player of all time - an extraordinary self-destructive force of nature - tiny in stature, but a huge heart for the game.<br /><br />What we have here is a kind of Hallmark film about the dangers of gambling. Sure, he wins, he loses, he blows it all on sex, drugs, and more gambling we get it, but where is the real play - where is what made him the greatest card player of all time.<br /><br />Much too flat, and frankly boring in places, this gets a four because we get to learn something about Stu the man, but Stu the card player, nada.<br /><br />Nicely shot and presented up to a point this is the perfect example of how not to make a film about cards: honestly, ESPN\\'s coverage of the World Series is more watchable than this.<br /><br />A waste of a great chance.\"',\n",
       " 0)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "next(stream_docs(path='../data/interim/movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter_no_change=1)\n",
    "doc_stream = stream_docs(path='../data/interim/movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:30\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.885\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "source": [
    "## Experiment tracking with MLflow"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: 'baseline' does not exist. Creating a new experiment\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:30\n",
      "Accuracy: 0.885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pyprind\n",
    "\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "# Set tracking uri\n",
    "mlflow.set_tracking_uri(\"file://\" + \"/Volumes/partition/projects/learning/movieclassifier/mlruns\")\n",
    "# Set experiment\n",
    "mlflow.set_experiment(experiment_name=\"baselines\")\n",
    "\n",
    "# Tracking\n",
    "with mlflow.start_run() as run:\n",
    "    vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "    clf = SGDClassifier(loss='log', random_state=1, n_iter_no_change=1)\n",
    "    doc_stream = stream_docs(path='../data/interim/movie_data.csv')\n",
    "\n",
    "    classes = np.array([0, 1])\n",
    "    for _ in range(45):\n",
    "        X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "        if not X_train:\n",
    "            break\n",
    "        X_train = vect.transform(X_train)\n",
    "        clf.partial_fit(X_train, y_train, classes=classes)\n",
    "        pbar.update()\n",
    "\n",
    "\n",
    "    X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "    X_test = vect.transform(X_test)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print('Accuracy: %.3f' % score)\n",
    "\n",
    "    mlflow.log_metric(\"score\", score)\n",
    "    mlflow.sklearn.log_model(clf, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
